---
title: "Final_Project"
author: "Pradnya Patil"
date: "12/10/2021"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### Web Scrapping ######



```{r}

# Whenever you see libraries listed at the top of code, you should always make sure that you have them installed. To install a library--or a 'package' as they are often called--use the install.packages() function in R.
library(rvest)
library(tidyverse)
library(stringr) # to modify string 

# This is the primary URL from which you will extract other URLs containing content of interest
main.url <- read_html("https://www.wsj.com/?mod=wsjheader_logo")

# Using the selector gadget, identify the URLs of interest on the page, and then copy the xpath for pasting into th html_nodes function. By piping the output into html_attr() using "href," we collect just the URLs from the links identified using the selector gadget.
scrape.list <- html_nodes(main.url,xpath='//*[contains(concat( " ", @class, " " ), concat( " ", "WSJTheme--headline--7VCzo7Ay ", " " ))]') %>%
  html_nodes(xpath='h3')%>% 
  html_nodes(xpath='a')%>%  
  html_attr("href")

# Another variation without the pipe: scrape.list <- html_attr(scrape.list, "href")
# By using the %>% we avoid the needless copy/paste to modify the variable containing our data


 #scrape.list


# Creates an empty vector that will be filled data by the 'for loop' below for Title, Author and Text
page.title <- vector()
page.date <- vector()
page.text <- vector()


# The for loop visits each URL in scrape.list and then collects the text content from each page, creating a new list
for (i in seq_along(scrape.list)) {
  new.url <- read_html(scrape.list[i])
  
  #Collects text content from pages
  text.add <- html_nodes(new.url, xpath='//p') %>%
    html_text()
  
  # Collapses all the separate <p> text content into one string of text
  text.add <- paste(text.add, collapse=" ")%>%str_trim()
  
  
  # Collects the title from pages
  text.title <- new.url %>% html_element("title")%>%
    html_text()%>%str_trim()
  
  # Collects the date from pages
  date.add <- html_nodes(new.url, 'time') %>%
    html_text()%>%str_replace_all("\n","")%>%str_trim()
  
  # Replace the missing dates with NA
  date.add <- ifelse(is.null(date.add),'No Date', date.add)
  
  page.text <- c(page.text, text.add)
  page.title <- c(page.title, text.title)
  page.date <- c(page.date, date.add)
}


# Using tibble, the list of URLs is combined with the text scraped from each URL to create a dataframe for our combined dataset
scrape.data <- tibble('URL'=scrape.list, 'Title'=page.title, 'Date'=page.date, 'Text'=page.text)


# Save data frame as a CSV file
write.csv(scrape.data, 'WSJ_News.csv')

```
####### Scrubbing & Wrangling ###########




```{r}
library(tidyverse)
library(tidytext)
library(ggplot2)


# read in csv file as tibble/data frame
scrape.data <- read.csv(file='C:/Users/patil/Desktop/IAL-620 NLP & text mining/WSJ_News.csv', stringsAsFactors=FALSE)

clean.data <- as_tibble(scrape.data)



# transform table into one-word-per-line tidytext format
clean.data <- clean.data %>%
  unnest_tokens(word, Text)

# most frequent words
clean.data %>%
  count(word, sort = TRUE)

# remove stop words
data(stop_words)
clean.data <- clean.data %>%
  anti_join(stop_words)

# check result of stop word removal
clean.data %>%
  count(word, sort = TRUE)

# remove numbers -- NOT from *Text Mining with R*
nums <- clean.data %>% filter(str_detect(word, "^[0-9]")) %>% select(word) %>% unique()

clean.data <- clean.data %>%
  anti_join(nums, by = "word")

# remove other words -- NOT from *Text Mining with R*
uni_sw <- data.frame(word = c("2021","copyright", "rights", "u.s"))

clean.data <- clean.data %>%
  anti_join(uni_sw, by = "word")

# visualize top words in corpus
clean.data %>%
  count(word, sort = TRUE) %>%
  filter(n > 80) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n,fill=word), ) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```

######### Sentiment Analysis ############





```{r}
library(tidyverse)
library(tidytext)
library(lubridate)
library(textdata)
library (stringr)
library(flipTime)

##############################################################
# continues to use the data restulging from rvest_example2.R
##############################################################

# read in csv file as tibble/data frame
scrape.data <- read.csv(file='C:/Users/patil/Desktop/IAL-620 NLP & text mining/WSJ_News.csv', stringsAsFactors=FALSE) %>%
  as_tibble()

# uses lubridate package to convert UTC datetime format
scrape.data$Date <- str_replace_all(scrape.data$Date,"Last Updated:","")
scrape.data$Date <- str_replace_all(scrape.data$Date,"Updated","")
scrape.data$Date <- str_replace_all(scrape.data$Date,"Published:","")
scrape.data$Date <- str_replace_all(scrape.data$Date,"Originally Published On","")
scrape.data$Date <- (str_trim(substr(str_trim(scrape.data$Date), 1, 13)))
scrape.data$Date[scrape.data$Date==""] <- NA
unique(scrape.data$Date)


# uses FlipTime package to convert Datetime format
scrape.data$Date <- AsDate(scrape.data$Date)

# filter to just 2020 and add month and week
tidy.data <- scrape.data %>%
  filter(year(Date) %in% c(2021)) %>%
  group_by(month=floor_date(Date, "month"), week = week(Date))

# transform month to month name abbreviation
tidy.data$month <- tidy.data$month %>%
  month(label = TRUE)

# explore
unique(tidy.data$month)
unique(tidy.data$week)

# transform to one word per line tidytext format
# this translates their book, line, chapter wrangling for months, weeks, and URLs
# chapter 2.2 Text Mining with R
tidy.data <- tidy.data %>%
  group_by(month) %>%
  ungroup() %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word, Text)

# capture negative sentiments for bing lexicon
# chapter 2.2 Text Mining with R
bing.negative <- get_sentiments("bing") %>%
  filter(sentiment == "negative")

# count the top bing lexicon negatives in a given month
# chapter 2.2 Text Mining with R
month.negative <- tidy.data %>%
  filter(month == "Feb") %>%
  inner_join(bing.negative) %>%
  count(word, sort = TRUE)

# reproduces figure 2.2 from Texting with R
url.sentiment <- tidy.data %>%
  inner_join(get_sentiments("bing")) %>%
  count(month, index = URL, sentiment) %>%
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

# reproduces figure 2.2 from Texting with R
ggplot(url.sentiment, aes(index, sentiment, fill = month)) +
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~month, ncol = 2, scales = "free_x")

# find out how much each word contributed to sentiment
# chapter 2.4 Text Minging with R
bing.word.counts <- tidy.data %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

# visualize top words contributing to sentiment
# figure 2.4 Text Minging with R
bing.word.counts %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(y = "Contribution to sentiment",
       x = NULL) +
  coord_flip()

##########################################
# bi-gram contenxt for sentiment analysis
##########################################

# tokenizing bigrams
# chapter 4.1.3 Text Mining with R
url.bigrams <- scrape.data %>%
  unnest_tokens(bigram, Text, token = "ngrams", n = 2)

# separating bigrams into two columns
# chapter 4.1.3 Text Mining with R
bigrams.separated <- url.bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# calling the AFINN lexicon for sentiment analysis
# chapter 4.1.3 Text Mining with R
AFINN <- get_sentiments("afinn")
# Enter an item from the menu, or 0 to exit
# Selection: Yes
# trying URL 'http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/6010/zip/imm6010.zip'
# Content type 'application/zip' length 16227 bytes (15 KB)
# downloaded 15 KB

# analyzing the most frequent words preceded by "not"
# chapter 4.1.3 Text Mining with R
not.words <- bigrams.separated %>%
  filter(word1 == "not") %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word2, value, sort = TRUE)

# explore
# chapter 4.1.3 Text Mining with R
not.words

# visualize "not" words
# figure 4.2 Text Mining with R
not.words %>%
  mutate(contribution = n * value) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution)) %>%
  ggplot(aes(word2, n * value, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  xlab("Words preceded by \"not\"") +
  ylab("Sentiment value * number of occurrences") +
  coord_flip()

# common negative words to precede other words
# chapter 4.1.3 Text Mining with R
negation.words <- c("not", "no", "never", "without")

# analyzing frequent words preceded by negation.words
# chapter 4.1.3 Text Mining with R
negated.words <- bigrams.separated %>%
  filter(word1 %in% negation.words) %>%
  inner_join(AFINN, by = c(word2 = "word")) %>%
  count(word1, word2, value, sort = TRUE)

# visualizing commonly negated words
# figure 4.3 Text Mining with R
negated.words %>%
  mutate(contribution = n * value,
         word2 = reorder(paste(word2, word1, sep = "__"), contribution)) %>%
  group_by(word1) %>%
  top_n(12, abs(contribution)) %>%
  ggplot(aes(word2, contribution, fill = n * value > 0)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ word1, scales = "free") +
  scale_x_discrete(labels = function(x) gsub("__.+$", "", x)) +
  xlab("Words preceded by negation term") +
  ylab("Sentiment value * # of occurrences") +
  coord_flip()

```

############ Word Frequencies, N-Grams, and word Correlations Assignments ###########







```{r}

library(tidyverse)
library(tidytext)
library(ggplot2)
library(tm)
library(stringr)
library(igraph)
library(ggraph)
library(widyr)

scrape.data <- read.csv(file='C:/Users/patil/Desktop/IAL-620 NLP & text mining/WSJ_News.csv', stringsAsFactors=FALSE) %>%
  as_tibble()

# removes carrige returns and new lines from text
scrape.data$Text <- gsub("\r?\n|\r", " ", scrape.data$Text)

# removes punctuation
scrape.data$Text <- gsub("[[:punct:]]", "", scrape.data$Text)

# forces entire corpus to lowercase
scrape.data$Text <- tolower(scrape.data$Text)

#removes numbers from text
scrape.data$Text <- removeNumbers(scrape.data$Text)

# remove stop words
scrape.data$Text <- removeWords(scrape.data$Text, stopwords("SMART"))

scrape.data$Text

# remove additional words
other.words <- c("2021","copyright", "rights", "u.s")

scrape.data$Text <- removeWords(scrape.data$Text, other.words)

# removes additional remaining whitespace
scrape.data$Text <- stripWhitespace(scrape.data$Text)

clean.data <- scrape.data %>%
  unnest_tokens(word, Text)

clean.data <- clean.data %>%
  count(word, sort = TRUE)

url.words <- scrape.data %>%
  unnest_tokens(word, Text) %>%
  count(URL, word, sort = TRUE)

total.words <- url.words %>%
  group_by(URL) %>%
  summarize(total = sum(n))

url.words <- left_join(url.words, total.words)

url.words <- url.words %>%
  bind_tf_idf(word, URL, n)

url.words <- url.words %>%
  select(-total) %>%
  arrange(desc(tf_idf))

# compare tf-idf to raw word frequencies for top 30 words
unique(url.words$word[1:30])
clean.data$word[1:30]

# raw bigrams
url.bigrams <- scrape.data %>%
  unnest_tokens(bigram, Text, token = "ngrams", n = 2)

url.bigrams %>%
  count(bigram, sort = TRUE)

# tf-idf bigrams
bigrams.separated <- url.bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigram.tf.idf <- url.bigrams %>%
  count(URL, bigram) %>%
  bind_tf_idf(bigram, URL, n) %>%
  arrange(desc(tf_idf))

clean.data$word[1:30]
unique(url.words$word[1:30])
bigram.tf.idf$bigram[1:30]

count_bigrams <- function(dataset) {
  dataset %>%
    unnest_tokens(bigram, Text, token = "ngrams", n = 2) %>%
    separate(bigram, c("word1", "word2"), sep = " ") %>%
    filter(!word1 %in% stop_words$word,
           !word2 %in% stop_words$word) %>%
    count(word1, word2, sort = TRUE)
}

visualize_bigrams <- function(bigrams) {
  set.seed(2016)
  a <- grid::arrow(type = "closed", length = unit(.15, "inches"))
  
  bigrams %>%
    graph_from_data_frame() %>%
    ggraph(layout = "fr") +
    geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a) +
    geom_node_point(color = "lightblue", size = 5) +
    geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
    theme_void()
}

# bring in your text data
viz.bigrams <- scrape.data %>%
  count_bigrams()

# filter out rare combinations, as well as digits
viz.bigrams %>%
  filter(n > 9,
         !str_detect(word1, "\\d"),
         !str_detect(word2, "\\d")) %>%
  
# display graph
  visualize_bigrams()

# transform data to one-token-per-document-per-row
clean.data2 <- scrape.data %>%
  unnest_tokens(word, Text)

# count words co-occurring in web pages
word_cors <- clean.data2 %>%
  group_by(word) %>%
  filter(n() >= 20) %>%
  pairwise_cor(word, URL, sort = TRUE)

# explore top word correlations
word_cors

word_cors %>%
  filter(item1 == "tools")


word_cors %>%
  filter(item1 %in% c("tools", "ads", "products", "membership")) %>%
  group_by(item1) %>%
  top_n(6) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip()

set.seed(2016)

word_cors %>%
  filter(correlation > .90) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = correlation), show.legend = FALSE) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), repel = TRUE) +
  theme_void()


```

########### Topic Modelling #############








```{r}
library(tidyverse)
library(tidytext)
library(topicmodels)
library(tm)
library(ldatuning)
library(lubridate)

##############################################################################################


# read in csv file as tibble/data frame
scrape.data <- read.csv(file='C:/Users/patil/Desktop/IAL-620 NLP & text mining/Web Scrapping Assignment/WSJ_News.csv', stringsAsFactors=FALSE)

# remove list of non-stop words that are common and hold no semantic value
other.words <- c("pm","slowest", "city", "Trump", "press", "pieces", "amazon",
                 "coronavirus", "loading", "incomes", "views", "Facebook", "wonderful", "post", "information",
                 "monday", "tuesday", "wednesday", "thursday", "friday", "saturday", "sunday")

# clean data
clean.data <- scrape.data$Text %>%
  removePunctuation() %>%
  removeNumbers() %>%
  tolower() %>%
  removeWords(stopwords("SMART")) %>%
  removeWords(other.words) %>%
  stripWhitespace()

# convert vector of scraped text to document term matrix using TM package
scrape.dtm <- VCorpus(VectorSource(clean.data)) %>%
  tm_map(stemDocument) %>%
  tm_map(stripWhitespace) %>%
  DocumentTermMatrix()

#######################################################################


# create a 2-topic LDA model and set seed so that output is predictable
scrape.lda <- LDA(scrape.dtm, k = 2, control = list(seed = 1234))

# explore
scrape.lda

# extract per topic word probabilities
scrape.topics <- tidy(scrape.lda, matrix = "beta")

# explore
scrape.topics

# organize data according to 'topic' for figure 6.2
scrape.top.terms <- scrape.topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# vizualize top 2 topics and their top 10 terms
# figure 6.2 in Text Mining with R
scrape.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

# calculate beta spread for figure 6.3
beta.spread <- scrape.topics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

# visualize words with greatest beta spread between 2 topics
# figure 6.3 in Text Mining with R
beta.spread %>%
  group_by(direction = log_ratio > 0) %>%
  top_n(10, abs(log_ratio)) %>%
  ungroup() %>%
  mutate(term = reorder(term, log_ratio)) %>%
  ggplot(aes(term, log_ratio)) +
  geom_col() +
  labs(y = "Log2 ratio of beta in topic 2 / topic 1") +
  coord_flip()

###############################################################
# decide number of topics intuitively or based on outside data
###############################################################

# reusing code from chapter 6.1.1 in Text Mining with R

# set seed and select number of topics with 'k'
scrape.lda <- LDA(scrape.dtm, k = 28, control = list(seed = 1234))

# extract per topic word probabilities
scrape.topics <- tidy(scrape.lda, matrix = "beta")

# organize data according to 'topic'
scrape.top.terms <- scrape.topics %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

# visualize top 5 terms for each topic
scrape.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()

########################################
# deciding number of topics empirically
########################################

# calculate metrics according to models
result <- FindTopicsNumber(
  scrape.dtm,
  topics = seq(from = 2, to = 40, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 14L,
  verbose = TRUE
)

# visualize results
FindTopicsNumber_plot(result)

```

############ Word Embeddings ################





```{r}
library(tidyverse)
library(tm)
library(text2vec)


# read in csv file as tibble/data frame
scrape.data <- read.csv(file='C:/Users/patil/Desktop/IAL-620 NLP & text mining/Web Scrapping Assignment/WSJ_News.csv', stringsAsFactors=FALSE)

###################
# pre-process text
###################

# use toString to transform vector of text documents to a single string of words
bulk.text <- toString(scrape.data$Text)

# use TM package to clean the dataset
clean.text <- bulk.text %>%
  removePunctuation() %>%
  removeNumbers() %>%
  tolower() %>%
  removeWords(stopwords("SMART")) %>%
  stripWhitespace()

# Create iterator over tokens
tokens = space_tokenizer(clean.text)

# Create vocabulary. Terms will be unigrams (simple words).
it = itoken(tokens, progressbar = FALSE)
vocab = create_vocabulary(it)

# reduce vocabulary to words with a minimum frequency of 5
vocab = prune_vocabulary(vocab, term_count_min = 5L)

# show number of words in vocab
length(vocab$term)

#####################################
# construct term co-occurance matrix
#####################################

# Use our filtered vocabulary
vectorizer = vocab_vectorizer(vocab)

# use window of 5 for context words
tcm = create_tcm(it, vectorizer, skip_grams_window = 5L)

# fitting our model using all available cores for processing
glove = GlobalVectors$new(rank = 50, x_max = 10)
wv_main = glove$fit_transform(tcm, n_iter = 10, convergence_tol = 0.01, n_threads = 8)

# explore
dim(wv_main)

# take a sum of main and context vector
wv_context = glove$components
word_vectors = wv_main + t(wv_context)
View(word_vectors)

#######################################
# explore context for individual words
#######################################

cos_sim = sim2(x = word_vectors, y = word_vectors["basketball", , drop = FALSE], method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)

###############################
# test word contexts/analogies
###############################

test.word = word_vectors["basketball", , drop = FALSE] -
  word_vectors["appeal", , drop = FALSE] +
  word_vectors["biden", , drop = FALSE]
cos_sim = sim2(x = word_vectors, y = test.word, method = "cosine", norm = "l2")
head(sort(cos_sim[,1], decreasing = TRUE), 5)



```